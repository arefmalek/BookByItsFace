{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal is to load an image\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 94 * 94, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,4)  #age (0 to 1 * 100), gender (<0.5 -> F, >=0.5 -> M), race ((0 to 1) / nraces) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 94 * 94)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(\"pth_files/mse_gender.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200, 3)\n",
      "(200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "#quick read of myself\n",
    "\n",
    "img = cv2.imread(\"arf.jpeg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "print(img.shape)\n",
    "img = cv2.resize(img, (200,200))\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 64,  69,  46],\n",
       "        [ 63,  68,  45],\n",
       "        [ 62,  67,  44],\n",
       "        ...,\n",
       "        [ 76,  84,  63],\n",
       "        [ 71,  79,  58],\n",
       "        [ 68,  76,  55]],\n",
       "\n",
       "       [[ 64,  69,  46],\n",
       "        [ 63,  68,  45],\n",
       "        [ 61,  66,  43],\n",
       "        ...,\n",
       "        [ 77,  85,  64],\n",
       "        [ 73,  81,  60],\n",
       "        [ 69,  77,  56]],\n",
       "\n",
       "       [[ 64,  70,  44],\n",
       "        [ 62,  68,  42],\n",
       "        [ 59,  65,  39],\n",
       "        ...,\n",
       "        [ 80,  88,  65],\n",
       "        [ 74,  82,  61],\n",
       "        [ 71,  79,  58]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[178, 173, 153],\n",
       "        [177, 172, 152],\n",
       "        [177, 172, 152],\n",
       "        ...,\n",
       "        [174, 173, 152],\n",
       "        [174, 173, 152],\n",
       "        [175, 174, 153]],\n",
       "\n",
       "       [[178, 173, 153],\n",
       "        [178, 173, 153],\n",
       "        [177, 172, 152],\n",
       "        ...,\n",
       "        [174, 173, 152],\n",
       "        [174, 173, 152],\n",
       "        [175, 174, 153]],\n",
       "\n",
       "       [[179, 174, 154],\n",
       "        [178, 173, 153],\n",
       "        [177, 172, 152],\n",
       "        ...,\n",
       "        [174, 173, 152],\n",
       "        [175, 174, 153],\n",
       "        [176, 175, 154]]], dtype=uint8)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_image = torch.from_numpy(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 64,  69,  46,  ...,  42,  51,  67],\n",
       "          [ 40,  47,  63,  ..., 120,  86,  90],\n",
       "          [120,  86,  88,  ...,  68,  76,  55],\n",
       "          ...,\n",
       "          [199, 169, 225,  ..., 110, 110,  84],\n",
       "          [ 68,  68,  44,  ..., 151, 137, 156],\n",
       "          [150, 139, 155,  ..., 133, 104, 227]],\n",
       "\n",
       "         [[188, 157, 222,  ..., 107, 107,  81],\n",
       "          [ 66,  66,  42,  ..., 154, 140, 157],\n",
       "          [149, 141, 154,  ...,  79,  47, 221],\n",
       "          ...,\n",
       "          [229, 225, 234,  ..., 232, 229, 223],\n",
       "          [232, 229, 222,  ..., 164, 146, 110],\n",
       "          [185, 170, 139,  ..., 229, 223, 232]],\n",
       "\n",
       "         [[229, 224, 233,  ..., 232, 229, 223],\n",
       "          [232, 229, 222,  ..., 163, 147, 111],\n",
       "          [181, 168, 134,  ..., 230, 224, 233],\n",
       "          ...,\n",
       "          [179, 174, 154,  ...,  80, 170, 123],\n",
       "          [ 81, 170, 123,  ..., 117,  66, 167],\n",
       "          [117,  66, 168,  ..., 176, 175, 154]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.reshape(tensor_image, (-1, ))\n",
    "torch.reshape(x, (-1,3,200,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.repeat(4,1)\n",
    "ti = torch.reshape(y, (4, 3, 200, 200)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = net(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5810, 0.5666, 0.6214, 0.5590]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5820, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
